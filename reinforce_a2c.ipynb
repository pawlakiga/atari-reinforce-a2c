{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforce_a2c.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "-14GvQKezAad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K60O-bVJprAm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from sympy.core import function\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import date, datetime\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global variables"
      ],
      "metadata": {
        "id": "mjmdupKczSGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG_PRINT = False\n",
        "REPLAY_BUFFER_LEN = 10000\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "BATCH_SIZE = 64\n",
        "NUM_BATCHES = 4\n",
        "STEPS_LIMIT = 1000\n",
        "LOG_PATH = ''\n",
        "\n",
        "CLASSIC_ENVIRONMENT_NAME = 'CartPole-v1'\n",
        "ATARI_ENVIRONMENT_NAME = 'DemonAttack-v4'\n",
        "\n",
        "policy_learning_rate = 1e-3\n",
        "value_learning_rate = 1e-3\n",
        "episodes_no = 500\n"
      ],
      "metadata": {
        "id": "YKA87-aczRNN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policies"
      ],
      "metadata": {
        "id": "nKN6UxVmzRcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def random_policy(action_space, state, model):\n",
        "    return action_space.sample()\n",
        "\n",
        "def network_model_policy(action_space, state, model):\n",
        "    action_probs = model(tf.expand_dims(state,0))\n",
        "    if DEBUG_PRINT:\n",
        "        print(f\"Action probs: {action_probs}\")\n",
        "    return np.random.choice(action_space.n, p=np.squeeze(action_probs))\n",
        "\n"
      ],
      "metadata": {
        "id": "bbygvwIvzDHV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Losses and metrics"
      ],
      "metadata": {
        "id": "nAqNicjhzwtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_loss(target, output_value):\n",
        "    return tf.multiply(-target, tf.math.log(output_value))\n",
        "\n",
        "\n",
        "def value_loss(target, output_value):\n",
        "    return tf.multiply(-target, output_value)\n",
        "\n",
        "def value_mse(target, output_value):\n",
        "    return tf.math.squared_difference(target, output_value)\n"
      ],
      "metadata": {
        "id": "bblYcVjYzxOk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network builders"
      ],
      "metadata": {
        "id": "fv6p0HlLz7RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dense_policy_network(input_shape, outputs_no):\n",
        "    x = tf.keras.layers.Input(shape = input_shape)\n",
        "    h = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='random_normal', bias_initializer='zeros')(x)\n",
        "    # h = layers.Dense(64, activation='relu', kernel_initializer='random_normal', bias_initializer='zeros')(h)\n",
        "    # h = layers.Dense(32, activation='relu')(h)\n",
        "    h = tf.keras.layers.Dense(outputs_no, kernel_initializer='random_normal', bias_initializer='zeros')(h)\n",
        "    y = tf.keras.layers.Softmax()(h)\n",
        "    model = Model(inputs=x,outputs=y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_policy_network(input_shape, outputs_no):\n",
        "    x = tf.keras.layers.Input(shape = input_shape)\n",
        "    h = tf.keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32)/255.)(x) \n",
        "    h = tf.keras.layers.Conv2D(filters = 64, kernel_size = (4,4), padding='same', activation = 'relu')(h)\n",
        "    h = tf.keras.layers.Conv2D(filters = 64, kernel_size = (8,8), padding='same',  activation = 'relu')(h)\n",
        "    # h = layers.Conv2D(filters = 64, kernel_size = (3,3), padding='same', activation='relu')(h)\n",
        "    h = tf.keras.layers.Flatten()(h)\n",
        "    h = tf.keras.layers.Dense(128, activation='relu')(h)\n",
        "    # h = layers.Dense(32, activation='relu')(h)\n",
        "    h = tf.keras.layers.Dense(outputs_no)(h)\n",
        "    y = tf.keras.layers.Softmax()(h)\n",
        "    model = Model(inputs = x, outputs = y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def dense_value_network(input_shape):\n",
        "    x = tf.keras.layers.Input(shape = input_shape)\n",
        "    h = tf.keras.layers.Dense(128, activation='relu', kernel_initializer='random_normal')(x)\n",
        "    # h = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='random_normal', bias_initializer='zeros')(h)\n",
        "    # h = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='random_normal', bias_initializer='zeros')(h)\n",
        "    y = tf.keras.layers.Dense(1)(h)\n",
        "    model = Model(inputs = x, outputs = y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def conv_value_network(input_shape):\n",
        "    x = tf.keras.layers.Input(shape = input_shape)\n",
        "    h = tf.keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32)/255.)(x)\n",
        "    h = tf.keras.layers.Conv2D(filters = 64, kernel_size = (4,4), padding='same')(h)\n",
        "    h = tf.keras.layers.Conv2D(filters = 64, kernel_size = (8,8), padding='same')(h)\n",
        "    h = tf.keras.layers.Flatten()(h)\n",
        "    h = tf.keras.layers.Dense(128, activation='relu')(h)\n",
        "    # h = tf.keras.layers.Dense(32, activation='relu')(h)\n",
        "    # h = tf.keras.layers.Dense(32, activation='relu')(h)\n",
        "    y = tf.keras.layers.Dense(1)(h)\n",
        "    model = Model(inputs = x, outputs = y)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "zc5_fQUcz7q4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network training functions"
      ],
      "metadata": {
        "id": "iHcw-S5i0HM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_training_step(model, states, actions, targets, optimizer):\n",
        "    \"\"\"\n",
        "    Training step for policy network, involving calculationa and application of gradients\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        output_values = model(states)\n",
        "        # Select output for the performed action\n",
        "        action_outputs = [output_values[i][actions[i]] for i in range(len(states))]\n",
        "        loss = policy_loss(targets, action_outputs)\n",
        "        if DEBUG_PRINT:\n",
        "            print(f\" |||| Policy |||| Targets: {targets}, outputs: {np.array(action_outputs)} and loss: {loss}\")\n",
        "\n",
        "    policy_grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(policy_grads, model.trainable_variables))\n",
        "    return tf.reduce_sum(loss)\n",
        "\n",
        "def value_training_step(model, states, targets, optimizer, metrics):\n",
        "    \"\"\"\n",
        "    training step for value network for reinforce with baseline\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        output_values = model(states)\n",
        "        # delta = G_t - v(S,w)\n",
        "        deltas = targets - output_values\n",
        "        loss = value_loss(deltas, output_values)\n",
        "        if DEBUG_PRINT:\n",
        "            print(f\" |||| Value |||| Targets: {targets}, outputs: {output_values}, deltas: {deltas} and loss: {loss}\")\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    metrics.update_state(model(states), tf.constant(deltas))\n",
        "    return tf.reduce_sum(loss)\n",
        "\n",
        "def a2c_value_training_step(model, states, rewards, next_states, dones,  optimizer, metrics, discount_factor):\n",
        "    \"\"\"\n",
        "    training step for value network in actor critic\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        # target = R + gamma * v(S',w) OR target = R if S' is terminal\n",
        "        targets = tf.add(tf.constant(rewards, dtype = 'float32'), tf.multiply(tf.multiply(tf.constant(discount_factor, dtype = 'float32'), model(next_states)),tf.constant(~dones,dtype='float32')))\n",
        "        output_values = model(states)\n",
        "        # delta = target - v(S,w)\n",
        "        deltas = targets - output_values\n",
        "        loss = value_loss(deltas, output_values)\n",
        "        if DEBUG_PRINT:\n",
        "            print(f\" |||| Value |||| Targets: {targets}, outputs: {output_values}, deltas: {deltas} and loss: {loss}\")\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    metrics.update_state(model(states), tf.constant(deltas))\n",
        "    return tf.reduce_sum(loss)"
      ],
      "metadata": {
        "id": "bOPobt8D0HiG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of Agent and ReplayBuffer classes"
      ],
      "metadata": {
        "id": "ScI35VHn0Nkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Class to represent replay buffer and sample from it\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_length) -> None:\n",
        "        self.buffer = deque(maxlen=max_length)\n",
        "\n",
        "    def add_experience(self, experience : tuple):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def get_random_experience_batch(self, batch_size):\n",
        "        \"\"\"\n",
        "        function to sample a random experience batch from buffer\n",
        "        Arguments:\n",
        "        @ batch_size - number of desired samples in batch\n",
        "        \"\"\"\n",
        "        sample = []\n",
        "        for i in range(batch_size):\n",
        "            sample.append(random.choice(self.buffer))\n",
        "        return [np.array([experience[field_index] for experience in sample]) for field_index in range(len(sample[0]))]\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()\n",
        "\n",
        "    @property\n",
        "    def len(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, environment, start_state):\n",
        "        self.state = start_state\n",
        "        self.environment = environment\n",
        "        self.state_history = []\n",
        "        self.action_history = []\n",
        "        self.rewards_history = []\n",
        "        self.replay_buffer = ReplayBuffer(max_length=REPLAY_BUFFER_LEN)\n",
        "\n",
        "    def take_action(self, policy, policy_model):\n",
        "        action = policy(action_space=self.environment.action_space, model = policy_model, state=self.state)\n",
        "        next_state, reward, done, info = self.environment.step(action)\n",
        "        if DEBUG_PRINT:\n",
        "            print(f\"Selected action: {action}, reward {reward}\")\n",
        "        self.state_history.append(self.state)\n",
        "        self.state = next_state\n",
        "        self.action_history.append(action)\n",
        "        self.rewards_history.append(reward)\n",
        "        return done\n",
        "\n",
        "    def play_episode(self, policy, policy_model, steps_limit=STEPS_LIMIT, experience_replay = False, discount_factor = 1):\n",
        "        done = False\n",
        "        for step in range(steps_limit):\n",
        "            done = self.take_action(policy=policy, policy_model=policy_model)\n",
        "            if done:\n",
        "                break\n",
        "        if experience_replay:\n",
        "            for t in range(len(self.state_history)):\n",
        "                G_t = tf.math.reduce_sum([self.rewards_history[tt] * discount_factor ** tt for tt in range(t + 1,\n",
        "                                                                                  len(self.state_history), 1)])\n",
        "                if t < len(self.state_history) - 1: # state is not terminal\n",
        "                    experience = (self.state_history[t], self.action_history[t], G_t, self.state_history[t+1], False)\n",
        "                else: \n",
        "                    experience = (self.state_history[t], self.action_history[t], G_t, [], True)\n",
        "                if DEBUG_PRINT:\n",
        "                    print(f\"Adding experience: {experience}\")\n",
        "                self.replay_buffer.add_experience(experience)\n",
        "            \n",
        "        return step\n",
        "    \n",
        "    def reset_episode(self, state):\n",
        "        self.state = state\n",
        "        self.state_history = []\n",
        "        self.action_history = []\n",
        "        self.rewards_history = []"
      ],
      "metadata": {
        "id": "l1cRJxQb0MfV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supporting functions\n",
        "\n"
      ],
      "metadata": {
        "id": "53sL_E5P0MDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def progress_bar(current, total, reward, bar_length=20):\n",
        "    fraction = current / total\n",
        "    arrow = int(fraction * bar_length - 1) * '-' + '>'\n",
        "    padding = int(bar_length - len(arrow)) * ' '\n",
        "    ending = '\\n' if current == total else '\\r'\n",
        "    print(f'Progress: [{arrow}{padding}] {int(fraction * 100)}%, cumulative reward: {reward}', end=ending)\n",
        "\n",
        "\n",
        "def save_to_file(data, file_path):\n",
        "    if not isinstance(file_path, str):\n",
        "        return \"\"\n",
        "    np.savetxt(file_path + '.csv', data, delimiter=',')\n",
        "\n",
        "\n",
        "def load_from_file(file_path):\n",
        "    return np.loadtxt(file_path, delimiter=',')\n",
        "\n",
        "\n",
        "def make_plot(x_data, y_data, x_label, y_label, title):\n",
        "    plt.plot(x_data, y_data)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def file_name(function_name : str, learning_rate, object, discount_factor = DISCOUNT_FACTOR, batch_size = BATCH_SIZE):\n",
        "    date_time = datetime.now()\n",
        "    dt_string = date_time.strftime(\"%d-%m_%H-%M\")\n",
        "    return f\"{object}\\\\{function_name}_lr{learning_rate}_gamma{discount_factor}_batch{batch_size}_{dt_string}.csv\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EWb6Dzgx6fVM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning functions"
      ],
      "metadata": {
        "id": "H4uUkaSY-l01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reinforce_classic(episodes_no = episodes_no,\n",
        "    policy_learning_rate = policy_learning_rate,\n",
        "    value_learning_rate = value_learning_rate,\n",
        "    discount_factor = DISCOUNT_FACTOR, \n",
        "    batch_size = BATCH_SIZE):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to perform the training of the agent with the REINFORCE algorithm in the classic environment with experience replay\n",
        "    \"\"\"\n",
        "\n",
        "    # Creation of file writer for tensorboard\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = f'{LOG_PATH}/reinforce_classic/lr{[value_learning_rate, policy_learning_rate]}_batch{BATCH_SIZE}_' + current_time + '/train'\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "    # Environment initialisation\n",
        "    env = gym.make(CLASSIC_ENVIRONMENT_NAME)\n",
        "    # Agent initialisation\n",
        "    agent = Agent(environment=env, start_state=env.reset())\n",
        "    # Policy network creation\n",
        "    policy_model = dense_policy_network(input_shape = env.observation_space.shape, outputs_no = env.action_space.n)\n",
        "    policy_optimizer = tf.keras.optimizers.Adam(learning_rate = policy_learning_rate)\n",
        "\n",
        "    rewards_mem = []\n",
        "    steps_mem = []\n",
        "\n",
        "    \n",
        "    for episode in range(episodes_no):\n",
        "        if DEBUG_PRINT:\n",
        "            print(f\"================== Episode {episode} =====================\")\n",
        "        step = agent.play_episode(policy = network_model_policy, policy_model= policy_model, experience_replay=True, discount_factor = discount_factor)\n",
        "        # After some episodes when we have enough samples in the buffer - start training\n",
        "        if episode >= 20:\n",
        "            total_loss = 0\n",
        "            states, actions, targets, _ , _ = agent.replay_buffer.get_random_experience_batch(batch_size = batch_size)\n",
        "            total_loss = policy_training_step(model = policy_model, states = states, actions = actions, targets = targets, optimizer=policy_optimizer)\n",
        "            # Write data to tensorboard\n",
        "            with train_summary_writer.as_default():\n",
        "                tf.summary.scalar('policy mean loss', total_loss/batch_size, step = episode)\n",
        "                tf.summary.scalar('cumulative reward', tf.reduce_sum(agent.rewards_history), step = episode)\n",
        "        rewards_mem.append(tf.reduce_sum(agent.rewards_history))\n",
        "        steps_mem.append(step)\n",
        "        agent.reset_episode(state = env.reset())\n",
        "        if not DEBUG_PRINT:\n",
        "            progress_bar(episode, episodes_no, rewards_mem[-1], 80)\n",
        "\n",
        "    env.close() \n",
        "    # make_plot(range(1, episodes_no + 1, 1), rewards_mem, 'Episode', 'Total reward', f'Cumulative reward for reinforce - {CLASSIC_ENVIRONMENT_NAME}')\n",
        "    np.savetxt(file_name('reinforce_classic\\\\reinforce_test_classic', policy_learning_rate, 'rewards', discount_factor=discount_factor, batch_size=batch_size), rewards_mem, delimiter=',')\n",
        "\n",
        "\n",
        "\n",
        "def reinforce_baseline_classic(episodes_no = episodes_no,\n",
        "                                policy_learning_rate = policy_learning_rate,\n",
        "                                value_learning_rate = value_learning_rate, \n",
        "                                discount_factor = DISCOUNT_FACTOR, \n",
        "                                batch_size = BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Function to perform the training of the agent with the REINFORCE algorithm in the classic environment with baseline and experience replay\n",
        "    \"\"\"\n",
        "\n",
        "    # Creation of file writer for tensorboard\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = f'{LOG_PATH}/baseline_classic/lr{[value_learning_rate, policy_learning_rate]}_batch{BATCH_SIZE}_' + current_time + '/train'\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "    # Environment initialisation\n",
        "    env = gym.make(CLASSIC_ENVIRONMENT_NAME)\n",
        "    # Agent initialisation\n",
        "    agent = Agent(environment=env, start_state=env.reset())\n",
        "    # Policy and value netowrks creation\n",
        "    value_model = dense_value_network(input_shape=env.observation_space.shape)\n",
        "    policy_model = dense_policy_network(input_shape = env.observation_space.shape, outputs_no = env.action_space.n)\n",
        "    # Optimizers and metrics initialisation\n",
        "    policy_optimizer = tf.keras.optimizers.Adam(learning_rate = policy_learning_rate)\n",
        "    value_optimizer = tf.keras.optimizers.Adam(learning_rate = value_learning_rate)\n",
        "    value_metrics = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "    rewards_mem = []\n",
        "    steps_mem = []\n",
        "\n",
        "    for episode in range(episodes_no):\n",
        "        step = agent.play_episode(policy = network_model_policy, policy_model= policy_model, experience_replay=True, discount_factor = discount_factor)\n",
        "        if episode >= 20:\n",
        "            states, actions, targets, _ , _ = agent.replay_buffer.get_random_experience_batch(batch_size = batch_size)\n",
        "            # Value network training step\n",
        "            total_value_loss = value_training_step(model = value_model, states = states, targets = targets, optimizer=value_optimizer, metrics=value_metrics)\n",
        "            # Calculation of baselines\n",
        "            baselines = value_model(states)\n",
        "            # delta = G_t - v(S,w)\n",
        "            deltas = tf.math.subtract(targets, baselines)\n",
        "            # Policy network training step\n",
        "            total_policy_loss = policy_training_step(model = policy_model, states = states, actions = actions, targets = deltas, optimizer=policy_optimizer)\n",
        "            # Writing data to tensorboard\n",
        "            with train_summary_writer.as_default():\n",
        "                tf.summary.scalar('policy mean loss', total_policy_loss/batch_size, step = episode)\n",
        "                tf.summary.scalar('value mean loss', total_value_loss/batch_size, step = episode)\n",
        "                tf.summary.scalar('cumulative reward', tf.reduce_sum(agent.rewards_history), step = episode)\n",
        "                tf.summary.scalar('value mse', value_metrics.result(), step=episode)\n",
        "\n",
        "        rewards_mem.append(tf.reduce_sum(agent.rewards_history))\n",
        "        steps_mem.append(step)\n",
        "        agent.reset_episode(state = env.reset())\n",
        "        if not DEBUG_PRINT:\n",
        "            progress_bar(episode, episodes_no, rewards_mem[-1], 80)\n",
        "\n",
        "    env.close() \n",
        "    make_plot(range(1, episodes_no + 1, 1), rewards_mem, 'Episode', 'Total reward', f'Cumulative reward for reinforce baseline - {CLASSIC_ENVIRONMENT_NAME}')\n",
        "    # np.savetxt(file_name('baseline_classic\\\\reinforce_baseline_test_classic', [value_learning_rate, policy_learning_rate], 'rewards', discount_factor=discount_factor, batch_size=batch_size), rewards_mem, delimiter=',')\n",
        "\n",
        "\n",
        "def reinforce_baseline_classic_no_batch(episodes_no = episodes_no, \n",
        "                                        value_learning_rate = value_learning_rate, \n",
        "                                        policy_learning_rate = policy_learning_rate, \n",
        "                                        discount_factor = DISCOUNT_FACTOR, \n",
        "                                        batch_size = BATCH_SIZE):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to perform the training of the agent with the REINFORCE algorithm in the classic environment\n",
        "    \"\"\"\n",
        "    # For tensorflow\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = f'{LOG_PATH}/baseline_classic_nb/lr{[value_learning_rate, policy_learning_rate]}_batch{BATCH_SIZE}_' + current_time + '/train'\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "    env = gym.make(CLASSIC_ENVIRONMENT_NAME)\n",
        "    agent = Agent(environment=env, start_state=env.reset())\n",
        "\n",
        "    # building networks\n",
        "    value_model = dense_value_network(input_shape=env.observation_space.shape)\n",
        "    policy_model = dense_policy_network(input_shape = env.observation_space.shape, outputs_no = env.action_space.n)\n",
        "\n",
        "    policy_optimizer = tf.keras.optimizers.Adam(learning_rate = policy_learning_rate)\n",
        "    value_optimizer = tf.keras.optimizers.Adam(learning_rate = value_learning_rate)\n",
        "    value_metrics = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "    rewards_mem = []\n",
        "    steps_mem = []\n",
        "\n",
        "    for episode in range(episodes_no):\n",
        "        if DEBUG_PRINT:\n",
        "            print(f\"================== Episode {episode} =====================\")\n",
        "        step = agent.play_episode(policy = network_model_policy, policy_model= policy_model, experience_replay=True, discount_factor = discount_factor)\n",
        "        total_loss = 0\n",
        "        value_loss = 0\n",
        "        G_t_mem = []\n",
        "        # learning \n",
        "\n",
        "        for t in range(len(agent.state_history)-1):\n",
        "            state_t = agent.state_history[t]\n",
        "            action_t = agent.action_history[t]\n",
        "            # calculate G_t\n",
        "            G_t = tf.math.reduce_sum(\n",
        "                    [agent.rewards_history[tt] * discount_factor ** tt for tt in range(t + 1, len(agent.state_history), 1)])\n",
        "            G_t_mem.append(G_t)\n",
        "            if DEBUG_PRINT:\n",
        "                print(f\"G_t: {G_t}\")\n",
        "            # value approximator training step\n",
        "            value_loss += value_training_step(model = value_model, \n",
        "                                            states = tf.expand_dims(state_t,0), \n",
        "                                            targets=(tf.expand_dims(G_t,0)), \n",
        "                                            optimizer= value_optimizer, \n",
        "                                            metrics = value_metrics)\n",
        "\n",
        "            # calculate baseline as current approximated state value\n",
        "            baseline = value_model(tf.expand_dims(state_t,0))\n",
        "            if DEBUG_PRINT:\n",
        "                print(f\"Value after: {baseline}\")\n",
        "            \n",
        "            delta_t = G_t - baseline\n",
        "            total_loss += policy_training_step(model = policy_model, states = tf.expand_dims(state_t,0), actions=tf.expand_dims(action_t,0), targets=tf.expand_dims(delta_t,0), optimizer=policy_optimizer )\n",
        "            if DEBUG_PRINT:\n",
        "                print(f\"Action probs after: {policy_model(tf.expand_dims(state_t,0))} and action was: {action_t}\")\n",
        "\n",
        "        mean_loss = total_loss / np.sum(G_t_mem)\n",
        "        value_mean_loss = value_loss/np.sum(G_t_mem)\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('policy mean loss', mean_loss, step = episode)\n",
        "            tf.summary.scalar('value mean loss', value_mean_loss, step = episode)\n",
        "            tf.summary.scalar('cumulative reward', tf.reduce_sum(agent.rewards_history), step = episode)\n",
        "            tf.summary.scalar('value mse', value_metrics.result(), step=episode)\n",
        "        \n",
        "        rewards_mem.append(tf.reduce_sum(agent.rewards_history))\n",
        "        steps_mem.append(step)\n",
        "        agent.reset_episode(state = env.reset())\n",
        "        if not DEBUG_PRINT:\n",
        "            progress_bar(episode, episodes_no, rewards_mem[-1], 80)\n",
        "\n",
        "    env.close() \n",
        "    make_plot(range(1, episodes_no + 1, 1), rewards_mem, 'Episode', 'Total reward', f'Cumulative reward for reinforce baseline - {CLASSIC_ENVIRONMENT_NAME}')\n",
        "    # np.savetxt(file_name('baseline_classic\\\\reinforce_baseline_test_classic_nb', [value_learning_rate, policy_learning_rate], 'rewards', discount_factor=discount_factor, batch_size=1), rewards_mem, delimiter=',')\n",
        "\n",
        "def reinforce_classic_no_batch(episodes_no = episodes_no, \n",
        "                                policy_learning_rate = policy_learning_rate,\n",
        "                                value_learning_rate = value_learning_rate, \n",
        "                                discount_factor = DISCOUNT_FACTOR, \n",
        "                                batch_size = BATCH_SIZE):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to perform the training of the agent with the REINFORCE algorithm in the classic environment with experience replay\n",
        "    \"\"\"\n",
        "\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = f'{LOG_PATH}/classic_nb/lr{[value_learning_rate, policy_learning_rate]}_batch{BATCH_SIZE}_' + current_time + '/train'\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "    env = gym.make(CLASSIC_ENVIRONMENT_NAME)\n",
        "    agent = Agent(environment=env, start_state=env.reset())\n",
        "\n",
        "    policy_model = dense_policy_network(input_shape = env.observation_space.shape, outputs_no = env.action_space.n)\n",
        "    policy_optimizer = tf.keras.optimizers.Adam(learning_rate = policy_learning_rate)\n",
        "\n",
        "    rewards_mem = []\n",
        "    steps_mem = []\n",
        "\n",
        "    for episode in range(episodes_no):\n",
        "        step = agent.play_episode(policy = network_model_policy, policy_model= policy_model, experience_replay=False)\n",
        "        total_loss = 0\n",
        "        G_t_mem = []\n",
        "\n",
        "        for t in range(len(agent.state_history)-1):\n",
        "            state_t = agent.state_history[t]\n",
        "            action_t = agent.action_history[t]\n",
        "            G_t = tf.math.reduce_sum(\n",
        "                    [agent.rewards_history[tt] * discount_factor ** tt for tt in range(t + 1, len(agent.state_history), 1)])\n",
        "\n",
        "            G_t_mem.append(G_t)\n",
        "            total_loss += policy_training_step(model = policy_model, states = tf.expand_dims(state_t,0), actions=tf.expand_dims(action_t,0), targets=tf.expand_dims(G_t,0), optimizer=policy_optimizer )\n",
        "            if DEBUG_PRINT:\n",
        "                print(f\"Action probs after: {policy_model(tf.expand_dims(state_t,0))} and action was: {action_t}, total_loss {total_loss}\")\n",
        "\n",
        "        mean_loss = total_loss / np.sum(G_t_mem)\n",
        "        if DEBUG_PRINT:\n",
        "            print(f\"And mean loss: {mean_loss}\")\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('policy mean loss', mean_loss, step = episode)\n",
        "            tf.summary.scalar('cumulative reward', tf.reduce_sum(agent.rewards_history), step = episode)\n",
        "\n",
        "        rewards_mem.append(tf.reduce_sum(agent.rewards_history))\n",
        "        steps_mem.append(step)\n",
        "        agent.reset_episode(state = env.reset())\n",
        "        if not DEBUG_PRINT:\n",
        "            progress_bar(episode, episodes_no, rewards_mem[-1], 80)\n",
        "\n",
        "    env.close() \n",
        "    make_plot(range(1, episodes_no + 1, 1), rewards_mem, 'Episode', 'Total reward', f'Cumulative reward for reinforce baseline - {CLASSIC_ENVIRONMENT_NAME}')\n",
        "    # np.savetxt(file_name('reinforce_classic\\\\reinforce_classic_nb', policy_learning_rate, 'rewards', discount_factor=discount_factor, batch_size=1), rewards_mem, delimiter=',')\n",
        "\n",
        "def reinforce_baseline_atari(episodes_no = episodes_no, \n",
        "                            policy_learning_rate = policy_learning_rate, \n",
        "                            value_learning_rate = value_learning_rate, \n",
        "                            discount_factor = DISCOUNT_FACTOR, \n",
        "                            batch_size = BATCH_SIZE):\n",
        "  \n",
        "    \"\"\"\n",
        "    Function to perform the training of the agent with the REINFORCE algorithm in the atari environment with experience replay\n",
        "    \"\"\"\n",
        "   \n",
        "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = f'{LOG_PATH}/baseline_atari/lr{[value_learning_rate, policy_learning_rate]}_batch{BATCH_SIZE}_' + current_time + '/train'\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "    env = gym.make(ATARI_ENVIRONMENT_NAME, full_action_space = False)\n",
        "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "    env = gym.wrappers.FrameStack(env, 4)    \n",
        "    agent = Agent(environment=env, start_state=env.reset())\n",
        "\n",
        "    value_model = conv_value_network(input_shape=env.observation_space.shape)\n",
        "    policy_model = conv_policy_network(input_shape = env.observation_space.shape, outputs_no = env.action_space.n)\n",
        "\n",
        "    policy_optimizer = tf.keras.optimizers.Adam(learning_rate = policy_learning_rate)\n",
        "    value_optimizer = tf.keras.optimizers.Adam(learning_rate = value_learning_rate)\n",
        "    value_metrics = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "    rewards_mem = []\n",
        "    steps_mem = []\n",
        "\n",
        "    for episode in range(episodes_no):\n",
        "        step = agent.play_episode(policy = network_model_policy, policy_model= policy_model, experience_replay=True, discount_factor = discount_factor)\n",
        "        print(f\"Episode {episode+1}/{episodes_no} finished in {step} steps with reward {tf.reduce_sum(agent.rewards_history)}\")\n",
        "        if episode >= 0.05 * episodes_no:\n",
        "            states, actions, targets, _ , _ = agent.replay_buffer.get_random_experience_batch(batch_size = batch_size)\n",
        "            value_mean_loss = value_training_step(model = value_model, states = states, targets = targets, optimizer=value_optimizer, metrics=value_metrics)\n",
        "            baselines = value_model(states)\n",
        "            deltas = tf.math.subtract(targets, baselines)\n",
        "            policy_mean_loss = policy_training_step(model = policy_model, states = states, actions = actions, targets = deltas, optimizer=policy_optimizer)\n",
        "            with train_summary_writer.as_default():\n",
        "                tf.summary.scalar('policy mean loss', tf.reduce_mean(policy_mean_loss), step = episode)\n",
        "                tf.summary.scalar('value mean loss', tf.reduce_mean(value_mean_loss), step = episode)\n",
        "                tf.summary.scalar('cumulative reward', tf.reduce_sum(agent.rewards_history), step = episode)\n",
        "                tf.summary.scalar('value mse', value_metrics.result(), step=episode)\n",
        "\n",
        "        rewards_mem.append(tf.reduce_sum(agent.rewards_history))\n",
        "        steps_mem.append(step)\n",
        "        agent.reset_episode(state = env.reset())\n",
        "        # if not DEBUG_PRINT:\n",
        "        #     progress_bar(episode, episodes_no, rewards_mem[-1], 80)\n",
        "\n",
        "    env.close() \n",
        "    make_plot(range(1, episodes_no + 1, 1), rewards_mem, 'Episode', 'Total reward', f'Cumulative reward for reinforce baseline - {CLASSIC_ENVIRONMENT_NAME}')\n",
        "    # np.savetxt(file_name('reinforce_baseline_test_classic', policy_learning_rate, 'rewards'), rewards_mem, delimiter=',')\n",
        "    # value_model.save_weights(f'value_{value_learning_rate}')\n",
        "    # policy_model.save_weights(f'policy_{policy_learning_rate}')\n",
        "\n",
        "def reinforce_baseline_atari_nb(episodes_no = episodes_no,\n",
        "                                value_learning_rate = value_learning_rate, \n",
        "                                policy_learning_rate = policy_learning_rate, \n",
        "                                discount_factor = DISCOUNT_FACTOR, \n",
        "                                batch_size = BATCH_SIZE):\n",
        "\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = f'{LOG_PATH}/reinforce_baseline_atari' + current_time + '/train'\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "    \n",
        "    env = gym.make(ATARI_ENVIRONMENT_NAME, full_action_space = False)\n",
        "    env = gym.wrappers.ResizeObservation(env, (80, 80))\n",
        "    env = gym.wrappers.FrameStack(env, 4)    \n",
        "    agent = Agent(environment=env, start_state=env.reset())\n",
        "\n",
        "    value_model = conv_value_network(input_shape=env.observation_space.shape)\n",
        "    policy_model = conv_policy_network(input_shape = env.observation_space.shape, outputs_no = env.action_space.n)\n",
        "\n",
        "    policy_optimizer = tf.keras.optimizers.Adam(learning_rate = policy_learning_rate)\n",
        "    value_optimizer = tf.keras.optimizers.Adam(learning_rate = value_learning_rate)\n",
        "    value_metrics = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "    rewards_mem = []\n",
        "    steps_mem = []\n",
        "\n",
        "    for episode in range(episodes_no):\n",
        "        step = agent.play_episode(policy = network_model_policy, policy_model= policy_model, experience_replay=False, discount_factor = discount_factor)\n",
        "        print(f\"Episode {episode} finished in {step} steps with reward {tf.reduce_sum(agent.rewards_history)}\")\n",
        "        total_loss = 0\n",
        "        value_loss = 0\n",
        "        G_t_mem = []\n",
        "        for t in range(len(agent.state_history)-1):\n",
        "            state_t = agent.state_history[t]\n",
        "            action_t = agent.action_history[t]\n",
        "            G_t = tf.math.reduce_sum(\n",
        "                    [agent.rewards_history[tt] * discount_factor ** tt for tt in range(t + 1, len(agent.state_history), 1)])\n",
        "            G_t_mem.append(G_t)\n",
        "            value_loss+= value_training_step(model = value_model, \n",
        "                                            states = tf.expand_dims(state_t,0), \n",
        "                                            targets=(tf.expand_dims(G_t,0)), \n",
        "                                            optimizer= value_optimizer, \n",
        "                                            metrics = value_metrics)\n",
        "\n",
        "            baseline = value_model(tf.expand_dims(state_t,0))\n",
        "            if DEBUG_PRINT:\n",
        "                print(f\"Value after: {baseline}\")\n",
        "            delta_t = G_t - baseline\n",
        "            total_loss += policy_training_step(model = policy_model, \n",
        "                                               states = tf.expand_dims(state_t,0), \n",
        "                                               actions=tf.expand_dims(action_t,0), \n",
        "                                               targets=tf.expand_dims(delta_t,0), \n",
        "                                               optimizer=policy_optimizer )\n",
        "            if DEBUG_PRINT:\n",
        "                print(f\"Action probs after: {policy_model(tf.expand_dims(state_t,0))} and action was: {action_t}\")\n",
        "\n",
        "        mean_loss = total_loss / np.sum(G_t_mem)\n",
        "        value_mean_loss = value_loss/ np.sum(G_t_mem)\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('policy mean loss', tf.reduce_mean(mean_loss), step = episode)\n",
        "            tf.summary.scalar('value mean loss', tf.reduce_mean(value_mean_loss), step = episode)\n",
        "            tf.summary.scalar('cumulative reward', tf.reduce_sum(agent.rewards_history), step = episode)\n",
        "            tf.summary.scalar('value mse', value_metrics.result(), step=episode)\n",
        "\n",
        "        rewards_mem.append(tf.reduce_sum(agent.rewards_history))\n",
        "        steps_mem.append(step)\n",
        "        agent.reset_episode(state = env.reset())\n",
        "        if not DEBUG_PRINT:\n",
        "            progress_bar(episode, episodes_no, rewards_mem[-1], 80)\n",
        "\n",
        "    env.close() \n",
        "    make_plot(range(1, episodes_no + 1, 1), rewards_mem, 'Episode', 'Total reward', f'Cumulative reward for reinforce baseline - {ATARI_ENVIRONMENT_NAME}')\n",
        "    # np.savetxt(file_name('reinforce_baseline_test_atari', policy_learning_rate, 'rewards'), rewards_mem, delimiter=',')\n",
        "    # value_model.save_weights(f'value_{value_learning_rate}')\n",
        "    # policy_model.save_weights(f'policy_{policy_learning_rate}')\n",
        "\n",
        "def actor_critic_classic_no_batch(episodes_no = episodes_no, \n",
        "                                    value_learning_rate = value_learning_rate, \n",
        "                                    policy_learning_rate = policy_learning_rate, \n",
        "                                    discount_factor = DISCOUNT_FACTOR, \n",
        "                                    batch_size = BATCH_SIZE):\n",
        "\n",
        "\n",
        "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = f'{LOG_PATH}/actor_critic_nb' + current_time + '/train'\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "    env = gym.make(CLASSIC_ENVIRONMENT_NAME)\n",
        "    agent = Agent(environment=env, start_state=env.reset())\n",
        "\n",
        "    # building networks\n",
        "    value_model = dense_value_network(input_shape=env.observation_space.shape)\n",
        "    policy_model = dense_policy_network(input_shape = env.observation_space.shape, outputs_no = env.action_space.n)\n",
        "\n",
        "    policy_optimizer = tf.keras.optimizers.Adam(learning_rate = policy_learning_rate)\n",
        "    value_optimizer = tf.keras.optimizers.Adam(learning_rate = value_learning_rate)\n",
        "    value_metrics = tf.keras.metrics.MeanSquaredError()\n",
        "\n",
        "    rewards_mem = []\n",
        "    steps_mem = []\n",
        "\n",
        "    for episode in range(episodes_no):\n",
        "        if DEBUG_PRINT:\n",
        "            print(f\"================== Episode {episode} =====================\")\n",
        "        total_loss = 0\n",
        "        total_value_loss = 0\n",
        "        # play an episode while learning\n",
        "        for step in range(STEPS_LIMIT):\n",
        "\n",
        "            done = agent.take_action(policy=network_model_policy, policy_model=policy_model)\n",
        "            state = agent.state_history[-1] # S\n",
        "            action = agent.action_history[-1] # A taken in S\n",
        "            reward = agent.rewards_history[-1] # R received after taking action A in S\n",
        "             # if S' is not terminal\n",
        "            if not done:\n",
        "                # S' next state\n",
        "                next_state = agent.state \n",
        "                # target = R + gamma * v(S',w)\n",
        "                target = reward + discount_factor * value_model(tf.expand_dims(next_state,0))[0] \n",
        "            else:\n",
        "                target = reward\n",
        "            if DEBUG_PRINT:\n",
        "                print(f\"State: {state}, action {action}, reward {reward}\")\n",
        "                print(f\"State value before: {value_model(tf.expand_dims(state, 0))}, action probs before: {policy_model(tf.expand_dims(state, 0))}\")\n",
        "            total_value_loss += value_training_step(model = value_model, \n",
        "                states = tf.expand_dims(state,0), \n",
        "                targets= tf.expand_dims(target,0), \n",
        "                optimizer = value_optimizer, \n",
        "                metrics = value_metrics\n",
        "                )\n",
        "            # delta = R + gamma * v(S',w) - v(S,w')\n",
        "            delta = target - value_model(tf.expand_dims(state, 0))\n",
        "            total_loss += policy_training_step(model = policy_model,\n",
        "                states = tf.expand_dims(state,0),\n",
        "                actions = tf.expand_dims(action,0),\n",
        "                targets = tf.expand_dims(delta,0),\n",
        "                optimizer = policy_optimizer   \n",
        "                )\n",
        "            if DEBUG_PRINT:\n",
        "                print(f\"State value after: {value_model(tf.expand_dims(state, 0))}, action probs before: {policy_model(tf.expand_dims(state, 0))}\")\n",
        "            if done: \n",
        "                break\n",
        "\n",
        "\n",
        "        steps_mem.append(step)\n",
        "        rewards_mem.append(tf.reduce_sum(agent.rewards_history))\n",
        "        mean_loss = total_loss / np.sum(agent.rewards_history)\n",
        "        value_mean_loss = total_value_loss/np.sum(agent.rewards_history)\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('policy mean loss', mean_loss, step = episode)\n",
        "            tf.summary.scalar('value mean loss', value_mean_loss, step = episode)\n",
        "            tf.summary.scalar('cumulative reward', tf.reduce_sum(agent.rewards_history), step = episode)\n",
        "            tf.summary.scalar('value mse', value_metrics.result(), step=episode)\n",
        "\n",
        "        agent.reset_episode(state = env.reset())\n",
        "        if not DEBUG_PRINT:\n",
        "            progress_bar(episode, episodes_no, rewards_mem[-1], 80)\n",
        "            \n",
        "    env.close() \n",
        "    make_plot(range(1, episodes_no + 1, 1), rewards_mem, 'Episode', 'Total reward', f'Cumulative reward for actor critic- {CLASSIC_ENVIRONMENT_NAME}')\n",
        "    np.savetxt(file_name('a2c\\\\actor_critic_nb', [policy_learning_rate, value_learning_rate], 'rewards', discount_factor=discount_factor, batch_size=1), rewards_mem, delimiter=',')\n",
        "\n"
      ],
      "metadata": {
        "id": "wo7jRYbA-pCg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test code"
      ],
      "metadata": {
        "id": "GM9RFvPfMvBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "functions_dict = {\n",
        "    1 : reinforce_classic, \n",
        "    2 : reinforce_baseline_classic, \n",
        "    3 : reinforce_classic_no_batch, \n",
        "    4 : reinforce_baseline_classic_no_batch, \n",
        "    5 : reinforce_baseline_atari,\n",
        "    6 : actor_critic_classic_no_batch,\n",
        "}\n",
        "\n",
        "\n",
        "# fun = functions_dict[config.chosen_fun]\n",
        "# fun()\n",
        "# reinforce_baseline_atari(episodes_no=episodes_no, policy_learning_rate=policy_learning_rate, value_learning_rate=value_learning_rate)\n",
        "\n",
        "learning_rates = [1e-5, 1e-3]\n",
        "batch_sizes = [8, 64, 128]\n",
        "discount_factors = [0.8, 0.95, 0.99, 1]\n",
        "\n",
        "for fun_no in [1,2,3,6]:\n",
        "    for df in discount_factors:\n",
        "        for bs in batch_sizes:\n",
        "            for policy_lr in learning_rates:\n",
        "                for value_lr in learning_rates:\n",
        "               \n",
        "                        test_fun = functions_dict[fun_no]\n",
        "                        print(\"-------------------------------------------------------------------------------------------------------------------\")\n",
        "                        print(f\"Testing {test_fun.__name__}, with gamma = {df}, batch = {bs}, lrs = {policy_lr, value_lr}\")\n",
        "                        test_fun(episodes_no = episodes_no, \n",
        "                                value_learning_rate = value_lr, \n",
        "                                policy_learning_rate = policy_lr, \n",
        "                                discount_factor = df, \n",
        "                                batch_size = bs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaKmjT3vMxAl",
        "outputId": "f1c3bee4-47ed-4350-ef5d-514399675dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 8, lrs = (1e-05, 1e-05)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 8, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 8, lrs = (0.001, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 8, lrs = (0.001, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 64, lrs = (1e-05, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 64, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 64, lrs = (0.001, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 64, lrs = (0.001, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 128, lrs = (1e-05, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 128, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 128, lrs = (0.001, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.8, batch = 128, lrs = (0.001, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 8, lrs = (1e-05, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 8, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 8, lrs = (0.001, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 8, lrs = (0.001, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 64, lrs = (1e-05, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 64, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 64, lrs = (0.001, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 64, lrs = (0.001, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 128, lrs = (1e-05, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 128, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 128, lrs = (0.001, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.95, batch = 128, lrs = (0.001, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 8, lrs = (1e-05, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 8, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 8, lrs = (0.001, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 8, lrs = (0.001, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 64, lrs = (1e-05, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 64, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 64, lrs = (0.001, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 64, lrs = (0.001, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 128, lrs = (1e-05, 1e-05)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 128, lrs = (1e-05, 0.001)\n",
            "-------------------------------------------------------------------------------------------------------------------\n",
            "Testing reinforce_classic, with gamma = 0.99, batch = 128, lrs = (0.001, 1e-05)\n"
          ]
        }
      ]
    }
  ]
}